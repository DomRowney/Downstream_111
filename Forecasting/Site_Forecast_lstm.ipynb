{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from itertools import product\n",
    "## import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import sqlalchemy \n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "## Add the path of the functions folder\n",
    "current_dir = os.getcwd()  ## Gets the current working directory\n",
    "sub_dir = os.path.abspath(os.path.join(current_dir, '..'\n",
    "                                       , 'Functions'))\n",
    "sys.path.append(sub_dir)\n",
    "\n",
    "# Now you can import functions\n",
    "from db_secrets import SQL_107\n",
    "\n",
    "from visualisations import plot_prediction_error, plot_prediction_density_subplots\n",
    "\n",
    "from helpers import aggregate_sites, keras_calculate_accuracy, keras_calculate_baseline_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow sequential model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Concatenate, LSTM, Dense, Dropout\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn warnings off to keep notebook tidy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text for query\n",
    "with open(\"../Exploratory_Analysis/111_sql.sql\", \"r\") as file:\n",
    "    query_text = file.read()\n",
    "\n",
    "query_text = query_text.replace('REPLACE START DATE','2022-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an engine + connection\n",
    "engine = create_engine(SQL_107())\n",
    "conn = engine.connect()\n",
    "\n",
    "## Return data\n",
    "df_raw = pd.read_sql(query_text,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Makes working copy\n",
    "df = df_raw.copy()\n",
    "\n",
    "#df = df.sample(n=100000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Call Connect Time'\n",
    "         ,'Outcome Location Name'\n",
    "         ,'Bank Holiday'\n",
    "         , 'In_Out_Hours'\n",
    "         , 'Sub ICB Name'\n",
    "         ,'Outcome Type']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Round time to nearest hour\n",
    "df['Call Connect Time'] = df['Call Connect Time'].dt.round(freq='h')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replaces low frequency sites with 'OTHER SITE'\n",
    "df['Outcome Location Name'] = (df['Outcome Location Name']\n",
    "                               .apply(lambda x: aggregate_sites(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### binary outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Calls'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Outcome'] = df['Outcome Type'].transform(lambda x: 0 if x == 'No UEC Contact' else 1)\n",
    "df = df.drop(['Outcome Type'],axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reassemble data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ICB values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregates to one column per place per timestamp\n",
    "df_call = pd.pivot_table(df\n",
    "                        ,values = 'Calls'\n",
    "                        ,index = 'Call Connect Time'\n",
    "                        ,columns ='Sub ICB Name'\n",
    "                        ,aggfunc ='sum'\n",
    "                        ,fill_value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_call.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Site values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_site = df[df['Outcome']==1]\n",
    "\n",
    "df_site = df_site[[ 'Call Connect Time'\n",
    "         , 'Outcome Location Name'\n",
    "         , 'Outcome'\n",
    "         ,]].groupby([ pd.Grouper(key='Call Connect Time', freq='1h')\n",
    "         , 'Outcome Location Name']).agg('sum').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removes OTHER SITE\n",
    "df_site = df_site[~(df_site['Outcome Location Name']=='OTHER SITE')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_site.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times = df[['Call Connect Time'\n",
    "            ,'Bank Holiday'\n",
    "            , 'In_Out_Hours']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique timestamps and sites\n",
    "unique_timestamps = df['Call Connect Time'].unique()\n",
    "unique_sites = df_site['Outcome Location Name'].unique()\n",
    "\n",
    "# Create a complete cross join of every site with every timestamp\n",
    "complete_index = pd.DataFrame(product(unique_timestamps, unique_sites)\n",
    "                              , columns=['Call Connect Time'\n",
    "                                         , 'Outcome Location Name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge time features\n",
    "df = complete_index.merge(df_times, on='Call Connect Time', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge calls + places\n",
    "df = df.merge(df_call,on='Call Connect Time', how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge sites\n",
    "df = df.merge(df_site,on=['Call Connect Time'\n",
    "                           , 'Outcome Location Name'], how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### date time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Date time conversion to numeric\n",
    "df['year']    = df['Call Connect Time'].dt.year\n",
    "\n",
    "df['month sin'] = np.sin(df['Call Connect Time'].dt.month * (2*np.pi/12))\n",
    "df['month cos'] = np.cos(df['Call Connect Time'].dt.month * (2*np.pi/12))\n",
    "\n",
    "df['YearDay sin'] = np.sin(df['Call Connect Time'].dt.day_of_year * (2*np.pi/365))\n",
    "df['YearDay cos'] = np.cos(df['Call Connect Time'].dt.day_of_year * (2*np.pi/365))\n",
    "\n",
    "df['weekday sin'] = np.sin(df['Call Connect Time'].dt.weekday+1 * (2*np.pi/7))  # Monday=0, Sunday=6\n",
    "df['weekday cos'] = np.cos(df['Call Connect Time'].dt.weekday+1 * (2*np.pi/7))  # Monday=0, Sunday=6\n",
    "\n",
    "df['Hour sin'] = np.sin(df['Call Connect Time'].dt.hour * (2*np.pi/24))\n",
    "df['Hour cos'] = np.cos(df['Call Connect Time'].dt.hour * (2*np.pi/24))\n",
    "\n",
    "#df = df.drop('Call Connect Time',axis=1) \n",
    "\n",
    "df = df.set_index('Call Connect Time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One hot encodinng for boolean variables\n",
    "bool_mapping = {\n",
    "    'Yes': 1,\n",
    "    'No': 0,\n",
    "    'In Hours': 1,\n",
    "    'Out of Hours': 0\n",
    "}\n",
    "\n",
    "df.loc[:,'Is Bank Holiday'] = df['Bank Holiday'].map(bool_mapping)             \n",
    "df.loc[:,'In Hours'] = df['In_Out_Hours'].map(bool_mapping)\n",
    "df = df.drop(['Bank Holiday','In_Out_Hours'],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dummy variables from Outcome Location Name\t\n",
    "#df = pd.concat([df, pd.get_dummies(df['Outcome Location Name']\n",
    "#                                   ,dtype=int\n",
    "#                                   , prefix='Site')]\n",
    "#                ,axis=1)\n",
    "#df = df.drop('Outcome Location Name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a baseline mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_X = df.drop('Outcome',axis=1)# X = all  except the 'Outcome' column\n",
    "base_y = df['Outcome']# y = 'Outcome' column \n",
    "\n",
    "base_X_train, base_X_test, base_y_train, base_y_test = train_test_split(base_X\n",
    "                                                    , base_y \n",
    "                                                    , test_size = 0.25\n",
    "                                                    , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Joins outcome onto predictors\n",
    "base_df = pd.concat([base_X_train,base_y_train],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Columns we care about for baseline model\n",
    "group_cols = [\n",
    "    'month sin'\n",
    "    ,'month cos'    \n",
    "    ,'weekday sin'\n",
    "    ,'weekday cos'\n",
    "    ,'Hour sin'\n",
    "    ,'Hour cos'] + df.columns[df.columns.str.startswith('Site_')].to_list() ## sites\n",
    "\n",
    "## Mean value across baseline\n",
    "base_trained = (base_df[group_cols + ['Outcome']]\n",
    "                .groupby(group_cols)\n",
    "                .agg( Pred_Outcome=pd.NamedAgg(column=\"Outcome\"\n",
    "                                               , aggfunc=\"mean\"))\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_y_pred_train = pd.merge(base_X_train,base_trained,how='left',on=group_cols)['Pred_Outcome']\n",
    "base_y_pred_test = pd.merge(base_X_test,base_trained,how='left',on=group_cols)['Pred_Outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagged Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_data(df,col,one_hot=False,lags=[1,2,3,4,12,24,168]):\n",
    "    \"\"\"calcultes the lags for a dataframe column\"\"\"\n",
    "    \n",
    "    print(f' > Calculating lags for: {col}')\n",
    "    \n",
    "    lagged_features = pd.DataFrame()\n",
    "\n",
    "    for x in lags:\n",
    "        # Create lag features\n",
    "        lagged_features[f'{col}_Lag{x}'] = df[col].shift(x)  # Previous x\n",
    "        if one_hot & (x > 1):\n",
    "            # Calculate rolling statistics\n",
    "            lagged_features[f'{col}_mean{x}'] = df[col].rolling(window=x).mean()\n",
    "            lagged_features[f'{col}_STD{x}']  = df[col].rolling(window=x).std() \n",
    "            lagged_features[f'{col}_min{x}']  = df[col].rolling(window=x).min() \n",
    "            lagged_features[f'{col}_max{x}']  = df[col].rolling(window=x).max() \n",
    "            lagged_features[f'{col}_median{x}']  = df[col].rolling(window=x).median()\n",
    "            lagged_features[f'{col}_var{x}']  = df[col].rolling(window=x).var()\n",
    "\n",
    "    df = pd.concat([df, lagged_features], axis=1)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_lag = ['Outcome']\n",
    "cols_to_lag.extend(list(df_call.columns))\n",
    "\n",
    "for col in cols_to_lag:\n",
    "    df = lag_data(df,col,one_hot=True)\n",
    "\n",
    "one_hot_cols_to_lag = ['Is Bank Holiday', 'In Hours']\n",
    "\n",
    "for col in one_hot_cols_to_lag:\n",
    "    df = lag_data(df,col,one_hot=False)\n",
    "\n",
    "df = df[168:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome Location Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dummy variables from Outcome Location Name\t\n",
    "df = pd.concat([df, pd.get_dummies(df['Outcome Location Name']\n",
    "                                   ,dtype=int\n",
    "                                   , prefix='Site')]\n",
    "                ,axis=1)\n",
    "df = df.drop('Outcome Location Name', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df['Outcome Location Name'] == 'NORTHUMBRIA SPECIALIST EMERGENCY CARE HOSPITAL']\n",
    "#df = df.drop('Outcome Location Name',axis=1)\n",
    "columns_order = (['Outcome'] + \n",
    "                 #['Outcome Location Name'] + \n",
    "                 [col for col in df.columns if col not in  ['Outcome','Outcome Location Name']] )\n",
    "df = df[columns_order]\n",
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "test_train_split = 0.70\n",
    "train_size = int(len(df) * test_train_split)\n",
    "\n",
    "train_df = df[:train_size]\n",
    "test_df = df[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(train, test):\n",
    "    \"\"\"Scale data 0-1 based on min and max in training set\n",
    "        , excluding the first and second column\"\"\"\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = MinMaxScaler()\n",
    "    \n",
    "    # Select columns to scale\n",
    "    train_to_scale = train.iloc[:, 2:]\n",
    "    test_to_scale = test.iloc[:, 2:]\n",
    "    \n",
    "    # Apply the scaler to the selected columns\n",
    "    train_sc = sc.fit_transform(train_to_scale)\n",
    "    test_sc = sc.transform(test_to_scale)\n",
    "    \n",
    "    # Combine the unscaled first column with the scaled remaining columns\n",
    "    train_result = pd.concat([\n",
    "        train.iloc[:, :2],  # Unscaled columns\n",
    "        pd.DataFrame(train_sc, columns=train_to_scale.columns\n",
    "                     , index=train.index)  # Scaled columns\n",
    "    ], axis=1)\n",
    "    \n",
    "    test_result = pd.concat([\n",
    "        test.iloc[:, :2],  # Unscaled columns\n",
    "        pd.DataFrame(test_sc, columns=test_to_scale.columns\n",
    "                     , index=test.index)  # Scaled columns\n",
    "    ], axis=1)\n",
    "    \n",
    "    return train_result, test_result, sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scale X data\n",
    "train_df_sc, test_df_sc,scaler  = scale_data(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#series_ids = df['Outcome Location Name'].values\n",
    "#unique_series = df['Outcome Location Name'].unique()\n",
    "#series_map = {s: i for i, s in enumerate(unique_series)}\n",
    "#series_ids_encoded = np.array([series_map[s] for s in series_ids])\n",
    "\n",
    "#series_one_hot = to_categorical(series_ids_encoded\n",
    "#                                    , num_classes=len(unique_series))\n",
    "\n",
    "#t = df.iloc[:,list(range(1,df.shape[1]))].values\n",
    "#t = np.hstack([t, series_one_hot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_windowed(df                 \n",
    "                  ,window_length = 24*8\n",
    "                  ,batch_size = 256\n",
    "                  #,series_col = 'Outcome Location Name'\n",
    "                  ):\n",
    "\n",
    "    \"\"\"Make data into windowed tensor for timeseries prediction\"\"\"\n",
    "\n",
    "    ## Separate the series ID\n",
    "    #series_ids = df[series_col].values\n",
    "    \n",
    "    ## One-hot encode series IDs\n",
    "    #unique_series = df[series_col].unique()\n",
    "    #series_map = {s: i for i, s in enumerate(unique_series)}\n",
    "    #series_ids_encoded = np.array([series_map[s] for s in series_ids])\n",
    "    #series_one_hot = to_categorical(series_ids_encoded\n",
    "    #                                , num_classes=len(unique_series))\n",
    "\n",
    "    ## Separate features (X) and targets (y)\n",
    "    X = df.iloc[:,1:df.shape[1] ].values\n",
    "    #X = np.hstack([X, series_one_hot])\n",
    "    y = df.iloc[:,[0]]\n",
    "\n",
    "    ## makes data into windowed tensors \n",
    "    dataset = timeseries_dataset_from_array(\n",
    "        data=X,\n",
    "        targets=y,\n",
    "        sequence_length=window_length, \n",
    "        sampling_rate=1, #skip items\n",
    "        batch_size=batch_size, #group together sequences efficiently in memory\n",
    "    )\n",
    "\n",
    "    return dataset #, len(unique_series) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_sc.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sets derviation window length\n",
    "window_length = 24*8 # 8 days\n",
    "\n",
    "## Sets memory batch size\n",
    "batch_size = 256\n",
    "\n",
    "## makes data into windowed tensors \n",
    "#train_dataset, train_sites_N\n",
    "train_dataset   = make_windowed(train_df_sc\n",
    "                                             ,window_length\n",
    "                                             ,batch_size)\n",
    "                                             #,series_col = 'Outcome Location Name')\n",
    "\n",
    "#test_dataset, test_sites_N\n",
    "test_dataset    = make_windowed(test_df_sc\n",
    "                                            ,window_length\n",
    "                                            ,batch_size)\n",
    "                                            #,series_col = 'Outcome Location Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.batch(32)._structure[0])\n",
    "print(train_dataset.batch(32)._structure[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_net(window_length,\n",
    "            number_features, \n",
    "            #number_series,\n",
    "            hidden_layers=1, \n",
    "            lstm_units=64,\n",
    "            hidden_layer_neurones=128, \n",
    "            dropout=0.0, \n",
    "            learning_rate=0.003):\n",
    "    \n",
    "    \"\"\"Make TensorFlow neural net\"\"\"\n",
    "    \n",
    "    ## Clear Tensorflow \n",
    "    K.clear_session()\n",
    "    \n",
    "    ## Set up neural net\n",
    "    net = Sequential()\n",
    "\n",
    "    ## input shape\n",
    "    #net.add(InputLayer((window_length,number_features + number_series)))\n",
    "    net.add(InputLayer((window_length,number_features)))\n",
    "\n",
    "    ## LSTM\n",
    "    net.add(LSTM(lstm_units,return_sequences = True))\n",
    "    net.add(LSTM(int(lstm_units/2)))\n",
    "\n",
    "    ## Add hidden hidden_layers using a loop\n",
    "    for i in range(hidden_layers):\n",
    "        # Add fully connected layer with ReLu activation\n",
    "        net.add(Dense(\n",
    "            hidden_layer_neurones, \n",
    "            input_dim=number_features,\n",
    "            activation='relu'))\n",
    "        # Add droput layer\n",
    "        net.add(Dropout(dropout))\n",
    "    \n",
    "    ## Add linear activation output\n",
    "    net.add(Dense(1, activation='linear'))  \n",
    "    #net.add(Dense(number_series, activation='linear'))  \n",
    "\n",
    "    #    net.add(Dense(1, activation='sigmoid'))    \n",
    "\n",
    "    ## Set optimiser\n",
    "    opt = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    ## Compiling model\n",
    "    net.compile(loss='mse', \n",
    "                optimizer=opt, \n",
    "                metrics=['mse','mae'])\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gets the number of features in the model\n",
    "for X_batch, y_batch in train_dataset.take(1): #takes 1 batch\n",
    "    print(f'X {X_batch.shape} | batch size={X_batch.shape[0]}, sequence length={X_batch.shape[1]}, features={X_batch.shape[2]}')\n",
    "    print(f'y {y_batch.shape} | batch size={y_batch.shape[0]}, outcomes={y_batch.shape[1]}')\n",
    "\n",
    "    number_features = X_batch.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_net(window_length = window_length,\n",
    "             number_features=number_features, \n",
    "             #number_series=1,\n",
    "             lstm_units = 128,\n",
    "             hidden_layers=1, \n",
    "             hidden_layer_neurones=128, \n",
    "             dropout=0.01, \n",
    "             learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarise the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define save checkpoint callback (only save if new best validation results)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    'model_checkpoint.keras', save_best_only=True)\n",
    "\n",
    "# Define early stopping callback\n",
    "# Stop when no validation improvement for 25 epochs\n",
    "# Restore weights to best validation accuracy\n",
    "early_stopping_cb_loss = keras.callbacks.EarlyStopping(\n",
    "    patience=25, restore_best_weights=True, monitor='val_loss')\n",
    "\n",
    "### Train model (and store training info in history)\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=10,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=test_dataset,\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpoint_cb\n",
    "                               , early_stopping_cb_loss\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to plot loss \n",
    "def visualise_loss(history, title):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualise_loss(history, \"Training and Validation Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get predictions\n",
    "test_predictions = model.predict(test_dataset).flatten()\n",
    "test_predictions = pd.DataFrame(data={'Predict':test_predictions})\n",
    "\n",
    "## get actuals\n",
    "test_actuals = test_df_sc.iloc[:len(test_predictions), [0]]\n",
    "test_actuals.sort_index(inplace=True)\n",
    "test_actuals['Mean']  = test_actuals['Outcome'].mean()\n",
    "\n",
    "## combine into dataframe\n",
    "test_predictions = pd.concat([\n",
    "        test_actuals, \n",
    "        test_predictions.set_index(test_actuals.index)\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot a sample of actual against predicted\n",
    "minplot = 1000\n",
    "maxplot = 1100\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(test_predictions['Outcome'][minplot:maxplot], label=\"Actual\")\n",
    "plt.plot(test_predictions['Predict'][minplot:maxplot], label=\"Prediction\")\n",
    "plt.plot(test_predictions['Mean'][minplot:maxplot], label=\"Mean Actual\")\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Add title and axis labels for context\n",
    "plt.title(\"TensorFlow LSTM: Predictions vs Actuals\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(plot_data, future, title):\n",
    "    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n",
    "    marker = [\".-\", \"rx\", \"go\"]\n",
    "\n",
    "    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n",
    "    future = list(range(future_steps))\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, val in enumerate(plot_data):\n",
    "        if i == 0:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "        else:            \n",
    "            plt.plot(future, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "\n",
    "    plt.legend()\n",
    "    #plt.xlim([time_steps[0], (future + 5) * 2])\n",
    "    plt.xlabel(\"Time-Step\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "future_steps = 24\n",
    "for x, y in test_dataset.take(5):\n",
    "    show_plot(\n",
    "        #[x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "        #1,\n",
    "        [x[0][:, 1].numpy(),\n",
    "          y[0].numpy()[:future_steps],\n",
    "            model.predict(x)[0][:future_steps]],\n",
    "        future_steps,\n",
    "        \"Single Step Prediction\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEGACY BEYOND HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxxxx STOP xxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_site_accuracy(df,model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Calculate and print accuracy at site level of training and test data fits\"\"\"    \n",
    "    \n",
    "    X_df = df.drop('Outcome',axis=1)\n",
    "    site_columns = X_df.columns[X_df.columns.str.startswith('Site_')]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for site in site_columns:\n",
    "        \n",
    "        # Get the column index for the site\n",
    "        site_idx = X_df.columns.get_loc(site)\n",
    "\n",
    "        # Filter array where site column equals 1\n",
    "        test_index = np.where(X_test[:, site_idx] == 1)[0]  \n",
    "        site_X_test = X_test[test_index]\n",
    "        site_y_test = y_test[test_index]\n",
    "\n",
    "        train_index = np.where(X_train[:, site_idx] == 1)[0]  \n",
    "        site_X_train = X_train[train_index]\n",
    "        site_y_train = y_train[train_index]\n",
    "\n",
    "        # Predict on training and test data\n",
    "        print(f'{site}: col {site_idx}')\n",
    "        site_y_pred_train = model.predict(site_X_train).flatten()\n",
    "        site_y_pred_test = model.predict(site_X_test).flatten()\n",
    "    \n",
    "        # Calculate Mean Absolute Error (MAE) for training and test sets\n",
    "        site_mae_train = np.mean(np.abs(site_y_pred_train - site_y_train))\n",
    "        site_mae_test = np.mean(np.abs(site_y_pred_test - site_y_test))\n",
    "        \n",
    "        # Calculate Mean Squared Error (MSE) for training and test sets\n",
    "        site_mse_train = np.mean((site_y_pred_train - site_y_train) ** 2)\n",
    "        site_mse_test = np.mean((site_y_pred_test - site_y_test) ** 2)\n",
    "\n",
    "        # Calculate Root Mean Squared Error (RMSE) for training and test sets\n",
    "        site_rmse_train = np.sqrt(site_mse_train)\n",
    "        site_rmse_test = np.sqrt(site_mse_test)\n",
    "\n",
    "        # Calculate NRMSE (Normalized RMSE)\n",
    "        range_y_train = np.max(site_y_train) - np.min(site_y_train)  # Range of y_train\n",
    "        range_y_test = np.max(site_y_test) - np.min(site_y_test)  # Range of y_test\n",
    "        site_nrmse_train = site_rmse_train / range_y_train\n",
    "        site_nrmse_test = site_rmse_test / range_y_test\n",
    "\n",
    "        # Calculate R^2 for training and test sets\n",
    "        ss_total_train = np.sum((site_y_train - np.mean(site_y_train)) ** 2)\n",
    "        ss_total_test = np.sum((site_y_test - np.mean(site_y_test)) ** 2)\n",
    "        ss_residual_train = np.sum((site_y_pred_train - site_y_train) ** 2)\n",
    "        ss_residual_test = np.sum((site_y_pred_test - site_y_test) ** 2)\n",
    "\n",
    "        r2_train = 1 - (ss_residual_train / ss_total_train)\n",
    "        r2_test = 1 - (ss_residual_test / ss_total_test)\n",
    "\n",
    "        ## results\n",
    "        site_result = {'Site':site\n",
    "                    ,'MAE train':site_mae_train                   \n",
    "                    ,'MAE test':site_mae_test\n",
    "                    ,'MSE train':site_mse_train\n",
    "                    ,'MSE test':site_mse_test\n",
    "                    ,'NRMSE train':site_nrmse_train\n",
    "                    ,'NRMSE test':site_nrmse_test\n",
    "                    ,'r2 train':r2_train\n",
    "                    ,'r2 test':r2_test\n",
    "                    }\n",
    "\n",
    "        results.append(site_result)\n",
    "\n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(history_dict,measure='mae'):\n",
    "    acc_values = history_dict[measure]\n",
    "    val_acc_values = history_dict[f'val_{measure}']\n",
    "    epochs = range(1, len(acc_values) + 1)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel(measure)\n",
    "\n",
    "    ax.plot(epochs, acc_values, color='blue', label=f'Training {measure}')\n",
    "    ax.plot(epochs, val_acc_values, color='red', label=f'Test {measure}')\n",
    "    ax.set_title(f'Training and validation {measure}')\n",
    "    \n",
    "    ax.legend()\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Baseline accuracy is the mean of site, month, weekday, hour\n",
    "keras_calculate_baseline_accuracy(base_y_pred_train\n",
    "                                ,base_y_pred_test\n",
    "                                ,base_y_train\n",
    "                                ,base_y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_calculate_accuracy(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_results = calculate_site_accuracy(df,model\n",
    "                                       , X_train\n",
    "                                       , X_test\n",
    "                                       , y_train\n",
    "                                       , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(history.history,measure='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred_train = model.predict(X_train).flatten()\n",
    "y_pred_test = model.predict(X_test).flatten()\n",
    "\n",
    "# Plot errors for both training and test data\n",
    "plot_prediction_error(y_train, y_pred_train, title='Training Data - Prediction Error Plot')\n",
    "plot_prediction_error(y_test, y_pred_test, title='Test Data - Prediction Error Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred_train = model.predict(X_train).flatten()\n",
    "y_pred_test = model.predict(X_test).flatten()\n",
    "\n",
    "# Plot the density plots as subplots\n",
    "plot_prediction_density_subplots(y_train, y_pred_train, y_test, y_pred_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_density_subplots(base_y_train\n",
    "                                 , base_y_pred_train\n",
    "                                 , base_y_test\n",
    "                                 , base_y_pred_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "downstream_111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
