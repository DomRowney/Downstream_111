{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "## import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlalchemy \n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "## Add the path of the functions folder\n",
    "current_dir = os.getcwd()  ## Gets the current working directory\n",
    "sub_dir = os.path.abspath(os.path.join(current_dir, '..'\n",
    "                                       , 'Functions'))\n",
    "sys.path.append(sub_dir)\n",
    "\n",
    "# Now you can import functions\n",
    "from db_secrets import SQL_107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow sequential model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn warnings off to keep notebook tidy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text for query\n",
    "with open(\"../Exploratory_Analysis/111_sql.sql\", \"r\") as file:\n",
    "    query_text = file.read()\n",
    "\n",
    "query_text = query_text.replace('REPLACE START DATE','2022-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an engine + connection\n",
    "engine = create_engine(SQL_107())\n",
    "conn = engine.connect()\n",
    "\n",
    "## Return data\n",
    "df_raw = pd.read_sql(query_text,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Makes working copy\n",
    "df = df_raw.copy()\n",
    "\n",
    "#df = df.sample(n=100000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Call Connect Time'\n",
    "         ,'Bank Holiday'\n",
    "         , 'In_Out_Hours'\n",
    "         , 'Sub ICB Name'\n",
    "         ,'Outcome Type']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### binary outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Calls'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Outcome'] = df['Outcome Type'].transform(lambda x: 0 if x == 'No UEC Contact' else 1)\n",
    "df = df.drop(['Outcome Type'],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Date time conversion to numeric\n",
    "df['Hour']    = df['Call Connect Time'].dt.hour\n",
    "df['year']    = df['Call Connect Time'].dt.year\n",
    "df['month']   = df['Call Connect Time'].dt.month\n",
    "df['day']     = df['Call Connect Time'].dt.day\n",
    "df['hour']    = df['Call Connect Time'].dt.hour\n",
    "df['weekday'] = df['Call Connect Time'].dt.weekday  # Monday=0, Sunday=6\n",
    "\n",
    "df = df.drop('Call Connect Time',axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregates count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby(['Hour'\n",
    "         , 'year'\n",
    "         , 'month'\n",
    "         , 'day'\n",
    "         , 'hour'\n",
    "         , 'weekday'\n",
    "         , 'Bank Holiday'\n",
    "         , 'In_Out_Hours'\n",
    "         , 'Sub ICB Name']).agg('sum').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One hot encodinng for boolean variables\n",
    "bool_mapping = {\n",
    "    'Yes': 1,\n",
    "    'No': 0,\n",
    "    'In Hours': 1,\n",
    "    'Out of Hours': 0\n",
    "}\n",
    "\n",
    "df.loc[:,'Is Bank Holiday'] = df['Bank Holiday'].map(bool_mapping)             \n",
    "df.loc[:,'In Hours'] = df['In_Out_Hours'].map(bool_mapping)\n",
    "df = df.drop(['Bank Holiday','In_Out_Hours'],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dummy variables from ICB\n",
    "df = pd.concat([df, pd.get_dummies(df['Sub ICB Name']\n",
    "                                   ,dtype=int\n",
    "                                   , prefix='SubICB')]\n",
    "                ,axis=1)\n",
    "df = df.drop('Sub ICB Name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Outcome',axis=1).to_numpy() # X = all  except the 'Outcome' column\n",
    "y = df['Outcome'].to_numpy() # y = 'Outcome' column \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X\n",
    "                                                    , y \n",
    "                                                    , test_size = 0.25\n",
    "                                                    , random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(Xy_train, Xy_test,X_or_y = ['X','y']):\n",
    "    \"\"\"Scale data 0-1 based on min and max in training set\"\"\"\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = MinMaxScaler()\n",
    "    \n",
    "    if X_or_y == 'X':\n",
    "        # Apply the scaler to the training and test sets\n",
    "        train_sc = sc.fit_transform(Xy_train)\n",
    "        test_sc = sc.transform(Xy_test)\n",
    "\n",
    "    elif X_or_y == 'y':\n",
    "        # Apply the scaler to the training and test sets\n",
    "        train_sc = sc.fit_transform(Xy_train.reshape(-1, 1))\n",
    "        test_sc = sc.transform(Xy_test.reshape(-1, 1))        \n",
    "        \n",
    "    return train_sc, test_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale X data\n",
    "X_train, X_test = scale_data(X_train, X_test, X_or_y='X')\n",
    "\n",
    "\n",
    "# Scale y data\n",
    "#y_train, y_test = scale_data(y_train, y_test, X_or_y='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_net(number_features, \n",
    "             hidden_layers=3, \n",
    "             hidden_layer_neurones=128, \n",
    "             dropout=0.0, \n",
    "             learning_rate=0.003):\n",
    "    \n",
    "    \"\"\"Make TensorFlow neural net\"\"\"\n",
    "    \n",
    "    # Clear Tensorflow \n",
    "    K.clear_session()\n",
    "    \n",
    "    # Set up neural net\n",
    "    net = Sequential()\n",
    "    \n",
    "    # Add hidden hidden_layers using a loop\n",
    "    for i in range(hidden_layers):\n",
    "        # Add fully connected layer with ReLu activation\n",
    "        net.add(Dense(\n",
    "            hidden_layer_neurones, \n",
    "            input_dim=number_features,\n",
    "            activation='relu'))\n",
    "        # Add droput layer\n",
    "        net.add(Dropout(dropout))\n",
    "    \n",
    "    # Add final sigmoid activation output\n",
    "    net.add(Dense(1, activation='linear'))    \n",
    "    #    net.add(Dense(1, activation='sigmoid'))    \n",
    "\n",
    "    # Compiling model\n",
    "    opt = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    net.compile(loss='mse', \n",
    "                optimizer=opt, \n",
    "                metrics=['mae'])\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test):\n",
    "    \"\"\"Calculate and print accuracy of training and test data fits\"\"\"    \n",
    "    \n",
    "   # Predict on training and test data\n",
    "    y_pred_train = model.predict(X_train_sc).flatten()\n",
    "    y_pred_test = model.predict(X_test_sc).flatten()\n",
    "\n",
    "    # Calculate Mean Absolute Error (MAE) for training and test sets\n",
    "    mae_train = np.mean(np.abs(y_pred_train - y_train))\n",
    "    mae_test = np.mean(np.abs(y_pred_test - y_test))\n",
    "    \n",
    "    # Calculate Mean Squared Error (MSE) for training and test sets\n",
    "    mse_train = np.mean((y_pred_train - y_train) ** 2)\n",
    "    mse_test = np.mean((y_pred_test - y_test) ** 2)\n",
    "\n",
    "    # Print the results\n",
    "    print(f'Training MAE: {mae_train:.3f}')\n",
    "    print(f'Test MAE: {mae_test:.3f}')\n",
    "    print(f'Training MSE: {mse_train:.3f}')\n",
    "    print(f'Test MSE: {mse_test:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(history_dict,measure='mae'):\n",
    "    acc_values = history_dict[measure]\n",
    "    val_acc_values = history_dict[f'val_{measure}']\n",
    "    epochs = range(1, len(acc_values) + 1)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel(measure)\n",
    "\n",
    "    ax.plot(epochs, acc_values, color='blue', label=f'Training {measure}')\n",
    "    ax.plot(epochs, val_acc_values, color='red', label=f'Test {measure}')\n",
    "    ax.set_title(f'Training and validation {measure}')\n",
    "    \n",
    "    ax.legend()\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_net(number_features=X_train.shape[1], \n",
    "             hidden_layers=3, \n",
    "             hidden_layer_neurones=128, \n",
    "             dropout=0.20, \n",
    "             learning_rate=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define save checkpoint callback (only save if new best validation results)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    'model_checkpoint.keras', save_best_only=True)\n",
    "\n",
    "# Define early stopping callback\n",
    "# Stop when no validation improvement for 25 epochs\n",
    "# Restore weights to best validation accuracy\n",
    "early_stopping_cb_loss = keras.callbacks.EarlyStopping(\n",
    "    patience=20, restore_best_weights=True, monitor='val_loss')\n",
    "\n",
    "#early_stopping_cb_acc = keras.callbacks.EarlyStopping(\n",
    "#    patience=5, restore_best_weights=True, monitor='val_accuracy')\n",
    "\n",
    "\n",
    "### Train model (and store training info in history)\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=200,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpoint_cb\n",
    "                               , early_stopping_cb_loss\n",
    "                               #, early_stopping_cb_acc\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(history.history,measure='mae')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
