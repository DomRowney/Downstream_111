{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "## import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import sqlalchemy \n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "## Add the path of the functions folder\n",
    "current_dir = os.getcwd()  ## Gets the current working directory\n",
    "sub_dir = os.path.abspath(os.path.join(current_dir, '..'\n",
    "                                       , 'Functions'))\n",
    "sys.path.append(sub_dir)\n",
    "\n",
    "# Now you can import functions\n",
    "from db_secrets import SQL_107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow sequential model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn warnings off to keep notebook tidy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text for query\n",
    "with open(\"../Exploratory_Analysis/111_sql.sql\", \"r\") as file:\n",
    "    query_text = file.read()\n",
    "\n",
    "query_text = query_text.replace('REPLACE START DATE','2022-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an engine + connection\n",
    "engine = create_engine(SQL_107())\n",
    "conn = engine.connect()\n",
    "\n",
    "## Return data\n",
    "df_raw = pd.read_sql(query_text,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Makes working copy\n",
    "df = df_raw.copy()\n",
    "\n",
    "#df = df.sample(n=100000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Call Connect Time'\n",
    "         ,'Bank Holiday'\n",
    "         , 'In_Out_Hours'\n",
    "         , 'Sub ICB Name'\n",
    "         ,'Outcome Type']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### binary outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Calls'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Outcome'] = df['Outcome Type'].transform(lambda x: 0 if x == 'No UEC Contact' else 1)\n",
    "df = df.drop(['Outcome Type'],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_s = df['Call Connect Time'].map(pd.Timestamp.timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Call Connect Time'].dt.day_of_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Date time conversion to numeric\n",
    "df['year']    = df['Call Connect Time'].dt.year\n",
    "\n",
    "df['month sin'] = np.sin(df['Call Connect Time'].dt.month * (2*np.pi/12))\n",
    "df['month cos'] = np.cos(df['Call Connect Time'].dt.month * (2*np.pi/12))\n",
    "\n",
    "df['YearDay sin'] = np.sin(df['Call Connect Time'].dt.day_of_year * (2*np.pi/365))\n",
    "df['YearDay cos'] = np.cos(df['Call Connect Time'].dt.day_of_year * (2*np.pi/365))\n",
    "\n",
    "df['weekday sin'] = np.sin(df['Call Connect Time'].dt.weekday+1 * (2*np.pi/7))  # Monday=0, Sunday=6\n",
    "df['weekday cos'] = np.cos(df['Call Connect Time'].dt.weekday+1 * (2*np.pi/7))  # Monday=0, Sunday=6\n",
    "\n",
    "df['Hour sin'] = np.sin(df['Call Connect Time'].dt.hour * (2*np.pi/24))\n",
    "df['Hour cos'] = np.cos(df['Call Connect Time'].dt.hour * (2*np.pi/24))\n",
    "\n",
    "df = df.drop('Call Connect Time',axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregates count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby([ 'year'\n",
    "         , 'month sin'\n",
    "         , 'month cos'\n",
    "         , 'YearDay sin'\n",
    "         , 'YearDay cos'\n",
    "         , 'weekday sin'\n",
    "         , 'weekday cos'\n",
    "         , 'Hour sin'\n",
    "         , 'Hour cos'\n",
    "         , 'Bank Holiday'\n",
    "         , 'In_Out_Hours'\n",
    "         , 'Sub ICB Name']).agg('sum').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One hot encodinng for boolean variables\n",
    "bool_mapping = {\n",
    "    'Yes': 1,\n",
    "    'No': 0,\n",
    "    'In Hours': 1,\n",
    "    'Out of Hours': 0\n",
    "}\n",
    "\n",
    "df.loc[:,'Is Bank Holiday'] = df['Bank Holiday'].map(bool_mapping)             \n",
    "df.loc[:,'In Hours'] = df['In_Out_Hours'].map(bool_mapping)\n",
    "df = df.drop(['Bank Holiday','In_Out_Hours'],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dummy variables from ICB\n",
    "df = pd.concat([df, pd.get_dummies(df['Sub ICB Name']\n",
    "                                   ,dtype=int\n",
    "                                   , prefix='SubICB')]\n",
    "                ,axis=1)\n",
    "df = df.drop('Sub ICB Name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a baseline mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_X = df.drop('Outcome',axis=1)# X = all  except the 'Outcome' column\n",
    "base_y = df['Outcome']# y = 'Outcome' column \n",
    "\n",
    "base_X_train, base_X_test, base_y_train, base_y_test = train_test_split(base_X\n",
    "                                                    , base_y \n",
    "                                                    , test_size = 0.25\n",
    "                                                    , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Joins outcome onto predictors\n",
    "base_df = pd.concat([base_X_train,base_y_train],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Columns we care about for baseline model\n",
    "group_cols = [\n",
    "    'month sin'\n",
    "    ,'month cos'    \n",
    "    ,'weekday sin'\n",
    "    ,'weekday cos'\n",
    "    ,'Hour sin'\n",
    "    ,'Hour cos'\n",
    "    ,'SubICB_County Durham'\n",
    "    ,'SubICB_Newcastle Gateshead'\n",
    "    ,'SubICB_North Tyneside'\n",
    "    ,'SubICB_Northumberland'\n",
    "    ,'SubICB_South Tyneside'\n",
    "    ,'SubICB_Sunderland'\n",
    "    ,'SubICB_Tees Valley'\n",
    "]\n",
    "\n",
    "## Mean value across baseline\n",
    "base_trained = (base_df[group_cols + ['Outcome']]\n",
    "                .groupby(group_cols)\n",
    "                .agg( Pred_Outcome=pd.NamedAgg(column=\"Outcome\"\n",
    "                                               , aggfunc=\"mean\"))\n",
    "                )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_y_pred_train = pd.merge(base_X_train,base_trained,how='left',on=group_cols)['Pred_Outcome']\n",
    "base_y_pred_test = pd.merge(base_X_test,base_trained,how='left',on=group_cols)['Pred_Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_baseline_accuracy(y_pred_train,y_pred_test,y_train, y_test):\n",
    "    \"\"\"Calculate and print accuracy of training and test data fits\"\"\"    \n",
    "\n",
    "    # Calculate Mean Absolute Error (MAE) for training and test sets\n",
    "    mae_train = np.mean(np.abs(y_pred_train - y_train))\n",
    "    mae_test = np.mean(np.abs(y_pred_test - y_test))\n",
    "    \n",
    "    # Calculate Mean Squared Error (MSE) for training and test sets\n",
    "    mse_train = np.mean((y_pred_train - y_train) ** 2)\n",
    "    mse_test = np.mean((y_pred_test - y_test) ** 2)\n",
    "\n",
    "    # Print the results\n",
    "    print(f'Training MAE: {mae_train:.3f}')\n",
    "    print(f'Test MAE: {mae_test:.3f}')\n",
    "    print(f'Training MSE: {mse_train:.3f}')\n",
    "    print(f'Test MSE: {mse_test:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Outcome',axis=1).to_numpy() # X = all  except the 'Outcome' column\n",
    "y = df['Outcome'].to_numpy() # y = 'Outcome' column \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X\n",
    "                                                    , y \n",
    "                                                    , test_size = 0.25\n",
    "                                                    , random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(Xy_train, Xy_test,X_or_y = ['X','y']):\n",
    "    \"\"\"Scale data 0-1 based on min and max in training set\"\"\"\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = MinMaxScaler()\n",
    "    \n",
    "    if X_or_y == 'X':\n",
    "        # Apply the scaler to the training and test sets\n",
    "        train_sc = sc.fit_transform(Xy_train)\n",
    "        test_sc = sc.transform(Xy_test)\n",
    "\n",
    "    elif X_or_y == 'y':\n",
    "        # Apply the scaler to the training and test sets\n",
    "        train_sc = sc.fit_transform(Xy_train.reshape(-1, 1))\n",
    "        test_sc = sc.transform(Xy_test.reshape(-1, 1))        \n",
    "        \n",
    "    return train_sc, test_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale X data\n",
    "X_train, X_test = scale_data(X_train, X_test, X_or_y='X')\n",
    "\n",
    "\n",
    "# Scale y data\n",
    "#y_train, y_test = scale_data(y_train, y_test, X_or_y='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_net(number_features, \n",
    "             hidden_layers=3, \n",
    "             hidden_layer_neurones=128, \n",
    "             dropout=0.0, \n",
    "             learning_rate=0.003):\n",
    "    \n",
    "    \"\"\"Make TensorFlow neural net\"\"\"\n",
    "    \n",
    "    # Clear Tensorflow \n",
    "    K.clear_session()\n",
    "    \n",
    "    # Set up neural net\n",
    "    net = Sequential()\n",
    "    \n",
    "    # Add hidden hidden_layers using a loop\n",
    "    for i in range(hidden_layers):\n",
    "        # Add fully connected layer with ReLu activation\n",
    "        net.add(Dense(\n",
    "            hidden_layer_neurones, \n",
    "            input_dim=number_features,\n",
    "            activation='relu'))\n",
    "        # Add droput layer\n",
    "        net.add(Dropout(dropout))\n",
    "    \n",
    "    # Add final sigmoid activation output\n",
    "    net.add(Dense(1, activation='linear'))    \n",
    "    #    net.add(Dense(1, activation='sigmoid'))    \n",
    "\n",
    "    # Compiling model\n",
    "    opt = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    net.compile(loss='mse', \n",
    "                optimizer=opt, \n",
    "                metrics=['mae'])\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test):\n",
    "    \"\"\"Calculate and print accuracy of training and test data fits\"\"\"    \n",
    "    \n",
    "   # Predict on training and test data\n",
    "    y_pred_train = model.predict(X_train_sc).flatten()\n",
    "    y_pred_test = model.predict(X_test_sc).flatten()\n",
    "\n",
    "    # Calculate Mean Absolute Error (MAE) for training and test sets\n",
    "    mae_train = np.mean(np.abs(y_pred_train - y_train))\n",
    "    mae_test = np.mean(np.abs(y_pred_test - y_test))\n",
    "    \n",
    "    # Calculate Mean Squared Error (MSE) for training and test sets\n",
    "    mse_train = np.mean((y_pred_train - y_train) ** 2)\n",
    "    mse_test = np.mean((y_pred_test - y_test) ** 2)\n",
    "\n",
    "    # Print the results\n",
    "    print(f'Training MAE: {mae_train:.3f}')\n",
    "    print(f'Test MAE: {mae_test:.3f}')\n",
    "    print(f'Training MSE: {mse_train:.3f}')\n",
    "    print(f'Test MSE: {mse_test:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(history_dict,measure='mae'):\n",
    "    acc_values = history_dict[measure]\n",
    "    val_acc_values = history_dict[f'val_{measure}']\n",
    "    epochs = range(1, len(acc_values) + 1)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel(measure)\n",
    "\n",
    "    ax.plot(epochs, acc_values, color='blue', label=f'Training {measure}')\n",
    "    ax.plot(epochs, val_acc_values, color='red', label=f'Test {measure}')\n",
    "    ax.set_title(f'Training and validation {measure}')\n",
    "    \n",
    "    ax.legend()\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_net(number_features=X_train.shape[1], \n",
    "             hidden_layers=3, \n",
    "             hidden_layer_neurones=128, \n",
    "             dropout=0.10, \n",
    "             learning_rate=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define save checkpoint callback (only save if new best validation results)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    'model_checkpoint.keras', save_best_only=True)\n",
    "\n",
    "# Define early stopping callback\n",
    "# Stop when no validation improvement for 25 epochs\n",
    "# Restore weights to best validation accuracy\n",
    "early_stopping_cb_loss = keras.callbacks.EarlyStopping(\n",
    "    patience=20, restore_best_weights=True, monitor='val_loss')\n",
    "\n",
    "#early_stopping_cb_acc = keras.callbacks.EarlyStopping(\n",
    "#    patience=5, restore_best_weights=True, monitor='val_accuracy')\n",
    "\n",
    "\n",
    "### Train model (and store training info in history)\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=200,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpoint_cb\n",
    "                               , early_stopping_cb_loss\n",
    "                               #, early_stopping_cb_acc\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Baseline accuracy is the mean of site, month, weekday, hour\n",
    "calculate_baseline_accuracy(base_y_pred_train\n",
    "                            ,base_y_pred_test\n",
    "                            ,base_y_train\n",
    "                            ,base_y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(history.history,measure='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_error(y_true, y_pred, title='Prediction Error Plot'):\n",
    "    \"\"\"Create a scatter plot comparing predicted and actual values, with an error line.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Scatter plot of actual vs predicted values\n",
    "    ax.scatter(y_true, y_pred, edgecolors=(0, 0, 0), alpha=0.7)\n",
    "    ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--', lw=2, color='red')\n",
    "    \n",
    "    ax.set_xlabel('Actual Values')\n",
    "    ax.set_ylabel('Predicted Values')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_train = model.predict(X_train).flatten()\n",
    "y_pred_test = model.predict(X_test).flatten()\n",
    "\n",
    "# Plot errors for both training and test data\n",
    "plot_prediction_error(y_train, y_pred_train, title='Training Data - Prediction Error Plot')\n",
    "plot_prediction_error(y_test, y_pred_test, title='Test Data - Prediction Error Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_density_subplots(y_train, y_pred_train, y_test, y_pred_test):\n",
    "    \"\"\"Create two subplots with shared axes and a common logarithmic color scale.\"\"\"\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6), sharex=True, sharey=True)\n",
    "    \n",
    "    # Create hexbin plot for training data\n",
    "    hb1 = ax[0].hexbin(y_train, y_pred_train, gridsize=50, cmap='Blues', mincnt=1, \n",
    "                       norm=mcolors.LogNorm())\n",
    "    ax[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \n",
    "               'k--', lw=2, color='red')\n",
    "    ax[0].set_title('Training Data - Prediction Density Plot')\n",
    "    ax[0].set_xlabel('Actual Values')\n",
    "    ax[0].set_ylabel('Predicted Values')\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    # Create hexbin plot for test data\n",
    "    hb2 = ax[1].hexbin(y_test, y_pred_test, gridsize=50, cmap='Blues', mincnt=1, \n",
    "                       norm=mcolors.LogNorm())\n",
    "    ax[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "               'k--', lw=2, color='red')\n",
    "    ax[1].set_title('Test Data - Prediction Density Plot')\n",
    "    ax[1].set_xlabel('Actual Values')\n",
    "    ax[1].grid(True)\n",
    "\n",
    "    # Add a single colorbar for both subplots\n",
    "    cb = fig.colorbar(hb1, ax=ax, orientation='vertical', fraction=0.03, pad=0.04)\n",
    "    cb.set_label('Log Density')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_train = model.predict(X_train).flatten()\n",
    "y_pred_test = model.predict(X_test).flatten()\n",
    "\n",
    "# Plot the density plots as subplots\n",
    "plot_prediction_density_subplots(y_train, y_pred_train, y_test, y_pred_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
