{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from itertools import product\n",
    "## import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import sqlalchemy \n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "## Add the path of the functions folder\n",
    "current_dir = os.getcwd()  ## Gets the current working directory\n",
    "sub_dir = os.path.abspath(os.path.join(current_dir, '..'\n",
    "                                       , 'Functions'))\n",
    "sys.path.append(sub_dir)\n",
    "\n",
    "# Now you can import functions\n",
    "from db_secrets import SQL_107\n",
    "\n",
    "from visualisations import plot_prediction_error, plot_prediction_density_subplots\n",
    "\n",
    "from helpers import aggregate_sites, keras_calculate_accuracy, keras_calculate_baseline_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow sequential model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn warnings off to keep notebook tidy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text for query\n",
    "with open(\"../Exploratory_Analysis/111_sql.sql\", \"r\") as file:\n",
    "    query_text = file.read()\n",
    "\n",
    "query_text = query_text.replace('REPLACE START DATE','2022-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an engine + connection\n",
    "engine = create_engine(SQL_107())\n",
    "conn = engine.connect()\n",
    "\n",
    "## Return data\n",
    "df_raw = pd.read_sql(query_text,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Makes working copy\n",
    "df = df_raw.copy()\n",
    "\n",
    "#df = df.sample(n=100000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Call Connect Time'\n",
    "         ,'Outcome Location Name'\n",
    "         ,'Bank Holiday'\n",
    "         , 'In_Out_Hours'\n",
    "         , 'Sub ICB Name'\n",
    "         ,'Outcome Type']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Round time to nearest hour\n",
    "df['Call Connect Time'] = df['Call Connect Time'].dt.round(freq='h')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replaces low frequency sites with 'OTHER SITE'\n",
    "df['Outcome Location Name'] = (df['Outcome Location Name']\n",
    "                               .apply(lambda x: aggregate_sites(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### binary outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Calls'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Outcome'] = df['Outcome Type'].transform(lambda x: 0 if x == 'No UEC Contact' else 1)\n",
    "df = df.drop(['Outcome Type'],axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reassemble data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ICB values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregates to one column per place per timestamp\n",
    "df_call = pd.pivot_table(df\n",
    "                        ,values = 'Calls'\n",
    "                        ,index = 'Call Connect Time'\n",
    "                        ,columns ='Sub ICB Name'\n",
    "                        ,aggfunc ='sum'\n",
    "                        ,fill_value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_call.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Site values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_site = df[df['Outcome']==1]\n",
    "\n",
    "df_site = df_site[[ 'Call Connect Time'\n",
    "         , 'Outcome Location Name'\n",
    "         , 'Outcome'\n",
    "         ,]].groupby([ pd.Grouper(key='Call Connect Time', freq='1h')\n",
    "         , 'Outcome Location Name']).agg('sum').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removes OTHER SITE\n",
    "df_site = df_site[~(df_site['Outcome Location Name']=='OTHER SITE')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_site.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times = df[['Call Connect Time'\n",
    "            ,'Bank Holiday'\n",
    "            , 'In_Out_Hours']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique timestamps and sites\n",
    "unique_timestamps = df['Call Connect Time'].unique()\n",
    "unique_sites = df_site['Outcome Location Name'].unique()\n",
    "\n",
    "# Create a complete cross join of every site with every timestamp\n",
    "complete_index = pd.DataFrame(product(unique_timestamps, unique_sites)\n",
    "                              , columns=['Call Connect Time'\n",
    "                                         , 'Outcome Location Name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge time features\n",
    "df = complete_index.merge(df_times, on='Call Connect Time', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge calls + places\n",
    "df = df.merge(df_call,on='Call Connect Time', how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge sites\n",
    "df = df.merge(df_site,on=['Call Connect Time'\n",
    "                           , 'Outcome Location Name'], how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### date time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Date time conversion to numeric\n",
    "df['year']    = df['Call Connect Time'].dt.year\n",
    "\n",
    "df['month sin'] = np.sin(df['Call Connect Time'].dt.month * (2*np.pi/12))\n",
    "df['month cos'] = np.cos(df['Call Connect Time'].dt.month * (2*np.pi/12))\n",
    "\n",
    "df['YearDay sin'] = np.sin(df['Call Connect Time'].dt.day_of_year * (2*np.pi/365))\n",
    "df['YearDay cos'] = np.cos(df['Call Connect Time'].dt.day_of_year * (2*np.pi/365))\n",
    "\n",
    "df['weekday sin'] = np.sin(df['Call Connect Time'].dt.weekday+1 * (2*np.pi/7))  # Monday=0, Sunday=6\n",
    "df['weekday cos'] = np.cos(df['Call Connect Time'].dt.weekday+1 * (2*np.pi/7))  # Monday=0, Sunday=6\n",
    "\n",
    "df['Hour sin'] = np.sin(df['Call Connect Time'].dt.hour * (2*np.pi/24))\n",
    "df['Hour cos'] = np.cos(df['Call Connect Time'].dt.hour * (2*np.pi/24))\n",
    "\n",
    "df = df.drop('Call Connect Time',axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One hot encodinng for boolean variables\n",
    "bool_mapping = {\n",
    "    'Yes': 1,\n",
    "    'No': 0,\n",
    "    'In Hours': 1,\n",
    "    'Out of Hours': 0\n",
    "}\n",
    "\n",
    "df.loc[:,'Is Bank Holiday'] = df['Bank Holiday'].map(bool_mapping)             \n",
    "df.loc[:,'In Hours'] = df['In_Out_Hours'].map(bool_mapping)\n",
    "df = df.drop(['Bank Holiday','In_Out_Hours'],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dummy variables from Outcome Location Name\t\n",
    "df = pd.concat([df, pd.get_dummies(df['Outcome Location Name']\n",
    "                                   ,dtype=int\n",
    "                                   , prefix='Site')]\n",
    "                ,axis=1)\n",
    "df = df.drop('Outcome Location Name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a baseline mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_X = df.drop('Outcome',axis=1)# X = all  except the 'Outcome' column\n",
    "base_y = df['Outcome']# y = 'Outcome' column \n",
    "\n",
    "base_X_train, base_X_test, base_y_train, base_y_test = train_test_split(base_X\n",
    "                                                    , base_y \n",
    "                                                    , test_size = 0.25\n",
    "                                                    , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Joins outcome onto predictors\n",
    "base_df = pd.concat([base_X_train,base_y_train],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Columns we care about for baseline model\n",
    "group_cols = [\n",
    "    'month sin'\n",
    "    ,'month cos'    \n",
    "    ,'weekday sin'\n",
    "    ,'weekday cos'\n",
    "    ,'Hour sin'\n",
    "    ,'Hour cos'] + df.columns[df.columns.str.startswith('Site_')].to_list() ## sites\n",
    "\n",
    "## Mean value across baseline\n",
    "base_trained = (base_df[group_cols + ['Outcome']]\n",
    "                .groupby(group_cols)\n",
    "                .agg( Pred_Outcome=pd.NamedAgg(column=\"Outcome\"\n",
    "                                               , aggfunc=\"mean\"))\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_y_pred_train = pd.merge(base_X_train,base_trained,how='left',on=group_cols)['Pred_Outcome']\n",
    "base_y_pred_test = pd.merge(base_X_test,base_trained,how='left',on=group_cols)['Pred_Outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Outcome',axis=1).to_numpy() # X = all  except the 'Outcome' column\n",
    "y = df['Outcome'].to_numpy() # y = 'Outcome' column \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X\n",
    "                                                    , y \n",
    "                                                    , test_size = 0.25\n",
    "                                                    , random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(Xy_train, Xy_test,X_or_y = ['X','y']):\n",
    "    \"\"\"Scale data 0-1 based on min and max in training set\"\"\"\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = MinMaxScaler()\n",
    "    \n",
    "    if X_or_y == 'X':\n",
    "        # Apply the scaler to the training and test sets\n",
    "        train_sc = sc.fit_transform(Xy_train)\n",
    "        test_sc = sc.transform(Xy_test)\n",
    "\n",
    "    elif X_or_y == 'y':\n",
    "        # Apply the scaler to the training and test sets\n",
    "        train_sc = sc.fit_transform(Xy_train.reshape(-1, 1))\n",
    "        test_sc = sc.transform(Xy_test.reshape(-1, 1))        \n",
    "        \n",
    "    return train_sc, test_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale X data\n",
    "X_train, X_test = scale_data(X_train, X_test, X_or_y='X')\n",
    "\n",
    "\n",
    "# Scale y data\n",
    "#y_train, y_test = scale_data(y_train, y_test, X_or_y='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_net(number_features, \n",
    "             hidden_layers=3, \n",
    "             hidden_layer_neurones=128, \n",
    "             dropout=0.0, \n",
    "             learning_rate=0.003):\n",
    "    \n",
    "    \"\"\"Make TensorFlow neural net\"\"\"\n",
    "    \n",
    "    # Clear Tensorflow \n",
    "    K.clear_session()\n",
    "    \n",
    "    # Set up neural net\n",
    "    net = Sequential()\n",
    "    \n",
    "    # Add hidden hidden_layers using a loop\n",
    "    for i in range(hidden_layers):\n",
    "        # Add fully connected layer with ReLu activation\n",
    "        net.add(Dense(\n",
    "            hidden_layer_neurones, \n",
    "            input_dim=number_features,\n",
    "            activation='relu'))\n",
    "        # Add droput layer\n",
    "        net.add(Dropout(dropout))\n",
    "    \n",
    "    # Add final sigmoid activation output\n",
    "    net.add(Dense(1, activation='linear'))    \n",
    "    #    net.add(Dense(1, activation='sigmoid'))    \n",
    "\n",
    "    # Compiling model\n",
    "    opt = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    net.compile(loss='mse', \n",
    "                optimizer=opt, \n",
    "                metrics=['mae'])\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_site_accuracy(df,model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Calculate and print accuracy at site level of training and test data fits\"\"\"    \n",
    "    \n",
    "    X_df = df.drop('Outcome',axis=1)\n",
    "    site_columns = X_df.columns[X_df.columns.str.startswith('Site_')]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for site in site_columns:\n",
    "        \n",
    "        # Get the column index for the site\n",
    "        site_idx = X_df.columns.get_loc(site)\n",
    "\n",
    "        # Filter array where site column equals 1\n",
    "        test_index = np.where(X_test[:, site_idx] == 1)[0]  \n",
    "        site_X_test = X_test[test_index]\n",
    "        site_y_test = y_test[test_index]\n",
    "\n",
    "        train_index = np.where(X_train[:, site_idx] == 1)[0]  \n",
    "        site_X_train = X_train[train_index]\n",
    "        site_y_train = y_train[train_index]\n",
    "\n",
    "        # Predict on training and test data\n",
    "        print(f'{site}: col {site_idx}')\n",
    "        site_y_pred_train = model.predict(site_X_train).flatten()\n",
    "        site_y_pred_test = model.predict(site_X_test).flatten()\n",
    "    \n",
    "        # Calculate Mean Absolute Error (MAE) for training and test sets\n",
    "        site_mae_train = np.mean(np.abs(site_y_pred_train - site_y_train))\n",
    "        site_mae_test = np.mean(np.abs(site_y_pred_test - site_y_test))\n",
    "        \n",
    "        # Calculate Mean Squared Error (MSE) for training and test sets\n",
    "        site_mse_train = np.mean((site_y_pred_train - site_y_train) ** 2)\n",
    "        site_mse_test = np.mean((site_y_pred_test - site_y_test) ** 2)\n",
    "\n",
    "        # Calculate Root Mean Squared Error (RMSE) for training and test sets\n",
    "        site_rmse_train = np.sqrt(site_mse_train)\n",
    "        site_rmse_test = np.sqrt(site_mse_test)\n",
    "\n",
    "        # Calculate NRMSE (Normalized RMSE)\n",
    "        range_y_train = np.max(site_y_train) - np.min(site_y_train)  # Range of y_train\n",
    "        range_y_test = np.max(site_y_test) - np.min(site_y_test)  # Range of y_test\n",
    "        site_nrmse_train = site_rmse_train / range_y_train\n",
    "        site_nrmse_test = site_rmse_test / range_y_test\n",
    "\n",
    "        # Calculate R^2 for training and test sets\n",
    "        ss_total_train = np.sum((site_y_train - np.mean(site_y_train)) ** 2)\n",
    "        ss_total_test = np.sum((site_y_test - np.mean(site_y_test)) ** 2)\n",
    "        ss_residual_train = np.sum((site_y_pred_train - site_y_train) ** 2)\n",
    "        ss_residual_test = np.sum((site_y_pred_test - site_y_test) ** 2)\n",
    "\n",
    "        r2_train = 1 - (ss_residual_train / ss_total_train)\n",
    "        r2_test = 1 - (ss_residual_test / ss_total_test)\n",
    "\n",
    "        ## results\n",
    "        site_result = {'Site':site\n",
    "                    ,'MAE train':site_mae_train                   \n",
    "                    ,'MAE test':site_mae_test\n",
    "                    ,'MSE train':site_mse_train\n",
    "                    ,'MSE test':site_mse_test\n",
    "                    ,'NRMSE train':site_nrmse_train\n",
    "                    ,'NRMSE test':site_nrmse_test\n",
    "                    ,'r2 train':r2_train\n",
    "                    ,'r2 test':r2_test\n",
    "                    }\n",
    "\n",
    "        results.append(site_result)\n",
    "\n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(history_dict,measure='mae'):\n",
    "    acc_values = history_dict[measure]\n",
    "    val_acc_values = history_dict[f'val_{measure}']\n",
    "    epochs = range(1, len(acc_values) + 1)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel(measure)\n",
    "\n",
    "    ax.plot(epochs, acc_values, color='blue', label=f'Training {measure}')\n",
    "    ax.plot(epochs, val_acc_values, color='red', label=f'Test {measure}')\n",
    "    ax.set_title(f'Training and validation {measure}')\n",
    "    \n",
    "    ax.legend()\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_net(number_features=X_train.shape[1], \n",
    "             hidden_layers=3, \n",
    "             hidden_layer_neurones=128, \n",
    "             dropout=0.10, \n",
    "             learning_rate=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define save checkpoint callback (only save if new best validation results)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    'model_checkpoint.keras', save_best_only=True)\n",
    "\n",
    "# Define early stopping callback\n",
    "# Stop when no validation improvement for 25 epochs\n",
    "# Restore weights to best validation accuracy\n",
    "early_stopping_cb_loss = keras.callbacks.EarlyStopping(\n",
    "    patience=20, restore_best_weights=True, monitor='val_loss')\n",
    "\n",
    "#early_stopping_cb_acc = keras.callbacks.EarlyStopping(\n",
    "#    patience=5, restore_best_weights=True, monitor='val_accuracy')\n",
    "\n",
    "\n",
    "### Train model (and store training info in history)\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=200,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpoint_cb\n",
    "                               , early_stopping_cb_loss\n",
    "                               #, early_stopping_cb_acc\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Baseline accuracy is the mean of site, month, weekday, hour\n",
    "keras_calculate_baseline_accuracy(base_y_pred_train\n",
    "                                ,base_y_pred_test\n",
    "                                ,base_y_train\n",
    "                                ,base_y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_calculate_accuracy(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_results = calculate_site_accuracy(df,model\n",
    "                                       , X_train\n",
    "                                       , X_test\n",
    "                                       , y_train\n",
    "                                       , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(history.history,measure='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred_train = model.predict(X_train).flatten()\n",
    "y_pred_test = model.predict(X_test).flatten()\n",
    "\n",
    "# Plot errors for both training and test data\n",
    "plot_prediction_error(y_train, y_pred_train, title='Training Data - Prediction Error Plot')\n",
    "plot_prediction_error(y_test, y_pred_test, title='Test Data - Prediction Error Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred_train = model.predict(X_train).flatten()\n",
    "y_pred_test = model.predict(X_test).flatten()\n",
    "\n",
    "# Plot the density plots as subplots\n",
    "plot_prediction_density_subplots(y_train, y_pred_train, y_test, y_pred_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_density_subplots(base_y_train\n",
    "                                 , base_y_pred_train\n",
    "                                 , base_y_test\n",
    "                                 , base_y_pred_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
