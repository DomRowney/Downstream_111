{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "## import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import sqlalchemy \n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "## Add the path of the functions folder\n",
    "current_dir = os.getcwd()  ## Gets the current working directory\n",
    "sub_dir = os.path.abspath(os.path.join(current_dir, '..'\n",
    "                                       , 'Functions'))\n",
    "sys.path.append(sub_dir)\n",
    "\n",
    "# Now you can import functions\n",
    "from db_secrets import SQL_107\n",
    "\n",
    "#from visualisations import plot_prediction_error, plot_prediction_density_subplots\n",
    "\n",
    "from helpers import aggregate_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-survival\n",
    "from sksurv.preprocessing import OneHotEncoder\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "\n",
    "from sksurv.metrics import (\n",
    "    concordance_index_censored,\n",
    "    concordance_index_ipcw,\n",
    "    cumulative_dynamic_auc,\n",
    "    integrated_brier_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text for query\n",
    "with open(\"../Exploratory_Analysis/111_sql.sql\", \"r\") as file:\n",
    "    query_text = file.read()\n",
    "\n",
    "query_text = query_text.replace('REPLACE START DATE','2022-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an engine + connection\n",
    "engine = create_engine(SQL_107())\n",
    "conn = engine.connect()\n",
    "\n",
    "## Return data\n",
    "df_raw = pd.read_sql(query_text,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Makes working copy\n",
    "df = df_raw.copy()\n",
    "\n",
    "#df = df.sample(n=100000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Call Connect Time'\n",
    "         ,'Outcome Location Name'\n",
    "         ,'Bank Holiday'\n",
    "         , 'In_Out_Hours'\n",
    "         , 'Sub ICB Name'\n",
    "         ,'Outcome Type'\n",
    "         ,'Outcome Datetime']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Uncensored'] = df['Outcome Type'].transform(lambda x: False if x == 'No UEC Contact' else True)\n",
    "df = df.drop(['Outcome Type'],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Round time to nearest minute\n",
    "df['Call Connect Time'] = df['Call Connect Time'].dt.round(freq='min')\n",
    "df['Outcome Datetime'] = df['Outcome Datetime'].dt.round(freq='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Mins to outcome'] = df['Outcome Datetime'] - df['Call Connect Time']\n",
    "df['Mins to outcome'] = df['Mins to outcome'].dt.total_seconds()/60\n",
    "\n",
    "## right censored data upto 24 hours\n",
    "df['Mins to outcome'] = df['Mins to outcome'].fillna(1441) ## minutes in day+1\n",
    "df['Mins to outcome'] = df['Mins to outcome'].transform(lambda x: 1441 if x > 1441 else x) \n",
    "\n",
    "df.loc[(df['Mins to outcome'] == 1441),'Uncensored' ] = False\n",
    "\n",
    "## removes zeros and less than zero\n",
    "df = df[df['Mins to outcome'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replaces low frequency sites with 'OTHER SITE'\n",
    "df['Outcome Location Name'] = (df['Outcome Location Name']\n",
    "                               .apply(lambda x: aggregate_sites(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## groups rare sites for a place\n",
    "positive_counts = df.groupby(['Sub ICB Name', 'Outcome Location Name'])['Uncensored'].sum().reset_index(name='Attends')\n",
    "\n",
    "total_positives = positive_counts.groupby('Sub ICB Name')['Attends'].sum().reset_index(name='Total_Attends')\n",
    "\n",
    "lu_site_agg = positive_counts.merge(total_positives, on='Sub ICB Name')\n",
    "lu_site_agg['Percentage'] = (lu_site_agg['Attends'] / lu_site_agg['Total_Attends']) * 100\n",
    "\n",
    "lu_site_agg['Location'] = 'OTHER SITE'\n",
    "\n",
    "## keep details of sites with > 5% of activity\n",
    "lu_site_agg.loc[(lu_site_agg['Percentage'] > 5) &\n",
    "                (lu_site_agg['Outcome Location Name'] != 'No UEC Contact')\n",
    "                , 'Location'] = lu_site_agg['Outcome Location Name']\n",
    "\n",
    "lu_site_agg.loc[lu_site_agg['Outcome Location Name'] == 'No UEC Contact'\n",
    "                , 'Location'] = 'No UEC Contact'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add new location\n",
    "df=pd.merge(df\n",
    "         ,lu_site_agg[['Sub ICB Name','Outcome Location Name','Location']]\n",
    "         , on = ['Sub ICB Name','Outcome Location Name']\n",
    "         , how='left')\n",
    "\n",
    "## Drop previous location\n",
    "df = df.drop('Outcome Location Name', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### date time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Date time conversion to numeric\n",
    "df['year']    = df['Call Connect Time'].dt.year\n",
    "\n",
    "df['month sin'] = np.sin(df['Call Connect Time'].dt.month * (2*np.pi/12))\n",
    "df['month cos'] = np.cos(df['Call Connect Time'].dt.month * (2*np.pi/12))\n",
    "\n",
    "df['YearDay sin'] = np.sin(df['Call Connect Time'].dt.day_of_year * (2*np.pi/365))\n",
    "df['YearDay cos'] = np.cos(df['Call Connect Time'].dt.day_of_year * (2*np.pi/365))\n",
    "\n",
    "df['weekday sin'] = np.sin(df['Call Connect Time'].dt.weekday+1 * (2*np.pi/7))  # Monday=0, Sunday=6\n",
    "df['weekday cos'] = np.cos(df['Call Connect Time'].dt.weekday+1 * (2*np.pi/7))  # Monday=0, Sunday=6\n",
    "\n",
    "df['Hour sin'] = np.sin(df['Call Connect Time'].dt.hour * (2*np.pi/24))\n",
    "df['Hour cos'] = np.cos(df['Call Connect Time'].dt.hour * (2*np.pi/24))\n",
    "\n",
    "df = df.drop('Call Connect Time',axis=1) \n",
    "df = df.drop('Outcome Datetime',axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Location'] = df['Location'].astype('category')\n",
    "df['Bank Holiday'] = df['Bank Holiday'].astype('category')\n",
    "df['In_Out_Hours'] = df['In_Out_Hours'].astype('category')\n",
    "df['Sub ICB Name'] = df['Sub ICB Name'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cox PH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a new df with one copy of the data per site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Outcome_Location = df[ ~df['Location'].isin(\n",
    "                            [ 'No UEC Contact', 'OTHER SITE']) ]['Location'].unique()\n",
    "print(Outcome_Location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()\n",
    "\n",
    "#for Location in Outcome_Location:\n",
    "#for Location in ['UNIVERSITY HOSPITAL OF NORTH DURHAM']:\n",
    "for Location in ['THE ROYAL VICTORIA INFIRMARY']:\n",
    "    print(Location)\n",
    "    temp_df = df.copy()\n",
    "    temp_df['Site Version'] = Location\n",
    "    temp_df['Uncensored'] = temp_df['Location'] == Location\n",
    "    temp_df.loc[(temp_df['Mins to outcome'] == 1441),'Uncensored' ] = False\n",
    "\n",
    "    new_df = pd.concat([new_df,temp_df], ignore_index=True, sort=False)\n",
    "\n",
    "new_df['Site Version'] = new_df['Site Version'].astype('category')\n",
    "\n",
    "## removes temp df\n",
    "del temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = new_df\n",
    "\n",
    "## removes temp df\n",
    "del new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_cols = ['Uncensored','Mins to outcome']\n",
    "\n",
    "X = df.drop(outcome_cols,axis=1)\n",
    "X = X.drop(['Location'],axis=1)\n",
    "y = df[outcome_cols]\n",
    "\n",
    "y = np.array(\n",
    "    list(y.itertuples(index=False, name=None)),  # Convert rows to tuples\n",
    "    dtype=[('Uncensored', '?'), ('Mins to outcome', '<f8')]  # Define the structured dtype\n",
    "    )\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X\n",
    "                                                    , y \n",
    "                                                    , stratify=y['Uncensored'] ## make sure there are equal proportions in test and train\n",
    "                                                    , test_size = 0.25\n",
    "                                                    , random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph = make_pipeline(OneHotEncoder(), CoxPHSurvivalAnalysis())\n",
    "cph.fit(X_train, y_train) ##takes 30+ mins\n",
    "cph_chf_funcs = cph.predict_cumulative_hazard_function(X_test, return_array=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrival_windows = np.arange(5, 1440, 5)\n",
    "\n",
    "cph_risk_scores = np.vstack([chf(arrival_windows) for chf in cph_chf_funcs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph_auc, cph_mean_auc = cumulative_dynamic_auc(survival_train=y_train\n",
    "                                               ,survival_test= y_test\n",
    "                                               ,estimate= cph_risk_scores\n",
    "                                               ,times= arrival_windows)\n",
    "\n",
    "plt.plot(arrival_windows, cph_auc, marker=\"o\")\n",
    "plt.axhline(cph_mean_auc, linestyle=\"--\")\n",
    "plt.xlabel(\"minutes from call\")\n",
    "plt.ylabel(\"time-dependent AUC\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordance\n",
    "The C-Index  measures the predictive accuracy of survival models by evaluating the proportion of concordant pairs relative to all comparable pairs within a dataset. A pair of subjects \n",
    "i and j is considered comparable if, given t_i < t_j, then δ_i=1. A pair of comparable subjects is concordant when the predicted mean time aligns with the actual event times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_index = concordance_index_censored(\n",
    "    y_test['Uncensored'],  # event indicator\n",
    "    y_test['Mins to outcome'],  # time to event\n",
    "    cph.predict(X_test)\n",
    ")\n",
    "\n",
    "print(\"Concordance Index:\", c_index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph_predictions = cph.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "downstream_111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
